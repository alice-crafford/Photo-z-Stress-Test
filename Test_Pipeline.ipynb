{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Essential Imports: \n",
    "import os\n",
    "import numpy as np\n",
    "import qp\n",
    "import tables_io\n",
    "from pathlib import Path \n",
    "from pzflow.examples import get_galaxy_data\n",
    "import ceci\n",
    "\n",
    "## RAIL-Specific Imports: \n",
    "import rail\n",
    "\n",
    "# old : from rail.creation.degradation import LSSTErrorModel, InvRedshiftIncompleteness\n",
    "from rail.creation.degradation.lsst_error_model import LSSTErrorModel\n",
    "from rail.creation.degradation.spectroscopic_degraders import InvRedshiftIncompleteness\n",
    "\n",
    "from rail.creation.engines.flowEngine import FlowModeler, FlowCreator, FlowPosterior\n",
    "from rail.core.data import TableHandle\n",
    "from rail.core.stage import RailStage\n",
    "from rail.core.utilStages import ColumnMapper, TableConverter\n",
    "\n",
    "# old : from rail.estimation.algos.flexzboost import Inform_FZBoost, FZBoost\n",
    "\n",
    "# from rail.estimation.algos.bpz_lite import BPZliteEstimator, BPZliteEstimator \n",
    "# from rail.estimation.algos.flexzboost import FlexZBoostInformer, FlexZBoostEstimator\n",
    "# from rail.estimation.algos.gpz import GPzEstimator, GPzEstimator \n",
    "# from rail.estimation.algos.knnpz import Inform_KNearNeighPDF, Inform_KNearNeighPDF \n",
    "# from rail.estimation.algos.minisom_som import MiniSOMInformer, MiniSOMInformer \n",
    "from rail.estimation.algos.pzflow_nf import PZFlowInformer, PZFlowEstimator \n",
    "# from rail.estimation.algos.sklearn_nn import Inform_SimpleNN, Inform_SimpleNN \n",
    "# from rail.estimation.algos.somoclu_som import SOMocluInformer, SOMocluInformer\n",
    "from rail.estimation.algos.train_z import TrainZEstimator, TrainZEstimator, TrainZEstimator\n",
    "\n",
    "\n",
    "from rail.evaluation.evaluator import Evaluator\n",
    "\n",
    "\n",
    "## Data Storage: \n",
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeModel():\n",
    "    #path to access the data \n",
    "    DATA_DIR = Path().resolve() / \"data\"\n",
    "    DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    catalog_file = DATA_DIR / \"base_catalog.pq\"\n",
    "\n",
    "    bands = ['u','g','r','i','z','y']\n",
    "    band_dict = {band:f'mag_{band}_lsst' for band in bands}\n",
    "    \n",
    "    #array of galaxies w/ 7 attributes for each: redshift & ugrizy\n",
    "    catalog = get_galaxy_data().rename(band_dict, axis=1) \n",
    "\n",
    "    #turns array into a table \n",
    "    tables_io.write(catalog, str(catalog_file.with_suffix(\"\")), catalog_file.suffix[1:])\n",
    "\n",
    "    catalog_file = str(catalog_file)\n",
    "    flow_file = str(DATA_DIR / \"trained_flow.pkl\")\n",
    "\n",
    "    print(flow_file)\n",
    "\n",
    "    #we set up the stage \n",
    "    flow_modeler_params = {\n",
    "        \"name\": \"flow_modeler\",\n",
    "        \"input\": catalog_file,\n",
    "        \"model\": flow_file,\n",
    "        \"seed\": 0,\n",
    "        \"phys_cols\": {\"redshift\": [0, 3]},\n",
    "        \"phot_cols\": {\n",
    "            \"mag_u_lsst\": [17, 35],\n",
    "            \"mag_g_lsst\": [16, 32],\n",
    "            \"mag_r_lsst\": [15, 30],\n",
    "            \"mag_i_lsst\": [15, 30],\n",
    "            \"mag_z_lsst\": [14, 29],\n",
    "            \"mag_y_lsst\": [14, 28],\n",
    "        },\n",
    "        \"calc_colors\": {\"ref_column_name\": \"mag_i_lsst\"},\n",
    "    }\n",
    "    flow_modeler = FlowModeler.make_stage(**flow_modeler_params)\n",
    "    # flow_modeler.fit_model()\n",
    "    return flow_modeler, flow_file ##.get_handle(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alicec03/Desktop/Summer_Research/Photo-z-Stress-Test/Photo-z-Stress-Test/data/trained_flow.pkl\n"
     ]
    }
   ],
   "source": [
    "modelData, flow_file = makeModel() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSet(ntrain, seed):\n",
    "    data = FlowCreator.make_stage(\n",
    "            name = 'train_set',\n",
    "            model = flow_file,\n",
    "            n_samples = ntrain,\n",
    "            seed = seed \n",
    "    )\n",
    "    return data #.sample(ntrain, seed)\n",
    "\n",
    "def invRedshift(pivot = 1.0):\n",
    "    degr = InvRedshiftIncompleteness.make_stage(\n",
    "        name = 'inv_redshift',\n",
    "        pivot_redshift = pivot\n",
    "    )\n",
    "    return degr #(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = FlowCreator.make_stage(\n",
    "#             name = 'train_set',\n",
    "#             model = flow_file,\n",
    "#             n_samples = 2,\n",
    "#             seed = 78 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origTrainData = trainSet(modelData, 100, 372)\n",
    "# bubble = origTrainData.sample(100, 372)\n",
    "\n",
    "# degTrainData = invRedshift(1.0)\n",
    "# dot = degTrainData(bubble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degTrainData.get_handle('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPosts(data, model, grid):\n",
    "    posts = FlowPosterior.make_stage(\n",
    "        name='get_posts'+str(data), \n",
    "        column='redshift',\n",
    "        grid = grid,\n",
    "        model = model,\n",
    "        data = data\n",
    "    )\n",
    "    return posts #posts.get_posterior(data, column = 'redshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeGrid(zmin, zmax, nbins):\n",
    "    import numpy as np\n",
    "    grid = np.linspace(zmin, zmax, nbins + 1)\n",
    "    return grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = makeGrid(0, 2.5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origTrainPosts = getPosts(origTrainData, modelData, grid)\n",
    "# degTrainPosts = getPosts(degTrainData, modelData, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run if you need output_orig_train_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_post_orig_train = FlowPosterior.make_stage(name='orig_train_posts', \n",
    "#                                              column='redshift',\n",
    "#                                              grid = np.linspace(0, 2.5, 101),\n",
    "#                                              model=flow_file,\n",
    "#                                              data = orig_train)\n",
    "\n",
    "# orig_train_pdfs = flow_post_orig_train.get_posterior(orig_train, column='redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run if you need output_deg_train_posts ** rerun this cell!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_post_deg_train = FlowPosterior.make_stage(name='deg_train_posts', \n",
    "#                                              column='redshift',\n",
    "#                                              grid = np.linspace(0, 2.5, 101),\n",
    "#                                              model=flow_file,\n",
    "#                                              err_samples = 0,\n",
    "#                                              data = deg_train)\n",
    "\n",
    "\n",
    "\n",
    "# deg_train_pdfs = flow_post_deg_train.get_posterior(deg_train, column='redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testSet(ntest, seed):\n",
    "    data = FlowCreator.make_stage(\n",
    "            name = 'test_set',\n",
    "            model = flow_file,\n",
    "            n_samples = ntest,\n",
    "            seed = seed \n",
    "    )\n",
    "    return data #.sample(ntest, seed)\n",
    "\n",
    "\n",
    "## you need to ask alex about where you can find the defaults for these params \n",
    "\n",
    "bands = ['u','g','r','i','z','y']\n",
    "band_dict = {band:f'mag_{band}_lsst' for band in bands}\n",
    "\n",
    "def lsstError(dict, seed): #tvis = 1, nYrObs = 1, airmass = 1, extendedSource = 1, sigmaSys = 1, magLim = 1, ndFlag = 1, A_min = 1, A_max = 1):\n",
    "    deg = LSSTErrorModel.make_stage(\n",
    "        name='lsst_error',\n",
    "        renameDict= dict, \n",
    "        ndFlag=np.nan,\n",
    "        seed=seed,\n",
    "    )\n",
    "    return deg #(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testSetMaker = testSet(modelData, 100, 17)\n",
    "# testData = testSetMaker.sample(100, 17)\n",
    "# degTestData = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run if you need output_orig_test_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_post_orig_test = FlowPosterior.make_stage(name='orig_test_posts', \n",
    "#                                              column='redshift',\n",
    "#                                              grid = np.linspace(0, 2.5, 101),\n",
    "#                                              model=flow_file,\n",
    "#                                              data = orig_test)\n",
    "\n",
    "# orig_test_pdfs = flow_post_orig_test.get_posterior(orig_test, column='redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run if you need output_deg_test_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_post_deg_test = FlowPosterior.make_stage(name='deg_test_posts', \n",
    "#                                              column='redshift',\n",
    "#                                              grid = np.linspace(0, 2.5, 101),\n",
    "#                                              model=flow_file,\n",
    "#                                              data = deg_test)\n",
    "\n",
    "# deg_test_pdfs = flow_post_deg_test.get_posterior(deg_test, column='redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def makeTable(datafile):\n",
    "    \n",
    "#     bands = ['u','g','r','i','z','y']\n",
    "#     rename_dict = {f'mag_{band}_lsst_err':f'mag_err_{band}_lsst' for band in bands}\n",
    "\n",
    "#     col_remapper = ColumnMapper.make_stage(\n",
    "#     name='col_remapper', \n",
    "#     columns=rename_dict,\n",
    "#     )\n",
    "#     table_conv = TableConverter.make_stage(\n",
    "#     name='table_conv', \n",
    "#     output_format='numpyDict',\n",
    "#     )\n",
    "#     pq = col_remapper(datafile)\n",
    "#     tabledata = table_conv(pq)\n",
    "#     table = tables_io.convertObj(tabledata.data, tables_io.types.PD_DATAFRAME)\n",
    "#     return table\n",
    "\n",
    "\n",
    "# ## make two separate functions for each stage, make bands, rename_dict inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ['u','g','r','i','z','y']\n",
    "band_dict_err = {f'mag_{band}_lsst_err':f'mag_err_{band}_lsst' for band in bands}\n",
    "\n",
    "def colRemapper(dict):\n",
    "    col_remapper = ColumnMapper.make_stage(\n",
    "    name='col_remapper', \n",
    "    columns=dict,\n",
    "    )\n",
    "    return col_remapper\n",
    "\n",
    "def tableConverter():\n",
    "    table_conv = TableConverter.make_stage(\n",
    "    name='table_conv', \n",
    "    output_format='numpyDict',\n",
    "    )\n",
    "    return table_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "squiggle = colRemapper(band_dict_err)\n",
    "noodle = tableConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainTable = makeTable(trainData)\n",
    "# testTable = makeTable(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inform & Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informFZBoost():\n",
    "    info = Inform_FZBoost.make_stage(\n",
    "    name ='inform_FZBoost', \n",
    "    model ='fzboost.pkl', \n",
    "    hdf5_groupname='',\n",
    "    )\n",
    "    # info.inform(data)\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# informedEst = informFZBoost()\n",
    "# informedEst.inform(degTrainData.get_handle('output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateFZBoost(info, nbins):\n",
    "    est = FZBoost.make_stage(\n",
    "    name='est_FZBoost', \n",
    "    nondetect_val=np.nan,\n",
    "    model= info, #.get_handle('model'), \n",
    "    hdf5_groupname='',\n",
    "    aliases=dict(input='test_data', output='fzboost_estim'),\n",
    "    nzbins = nbins \n",
    "    )\n",
    "    return est #.estimate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estData = estimateFZBoost(informedEst, 100)\n",
    "\n",
    "# estData.estimate(testSetMaker.get_handle('output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function connect_input in module rail.core.stage:\n",
      "\n",
      "connect_input(self, other, inputTag=None, outputTag=None)\n",
      "    Connect another stage to this stage as an input\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : RailStage\n",
      "         The stage whose output is being connected\n",
      "    inputTag : str\n",
      "         Which input tag of this stage to connect to.  None -> self.inputs[0]\n",
      "    outputTag : str\n",
      "         Which output tag of the other stage to connect to.  None -> other.outputs[0]\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    handle : The input handle for this stage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rail.core import RailStage\n",
    "\n",
    "help(RailStage.connect_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informPZFlow():\n",
    "    inf = PZFlowInformer.make_stage(\n",
    "    name = 'inform_PZFlow',\n",
    "    model = 'pzflow.pkl',\n",
    "    hdf5_groupname=\"\"\n",
    "    )\n",
    "    return inf\n",
    "\n",
    "def estimatePZFlow(info):\n",
    "    est = PZFlowEstimator.make_stage(\n",
    "    name = 'estimate_PZFlow',\n",
    "    model = 'pzflow.pkl', #info.get_handle('model'),\n",
    "    hdf5_groupname=\"\"\n",
    "    )\n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PZFlowEstimator in module rail.estimation.algos.pzflow_nf:\n",
      "\n",
      "class PZFlowEstimator(rail.estimation.estimator.CatEstimator)\n",
      " |  PZFlowEstimator(args, comm=None)\n",
      " |  \n",
      " |  CatEstimator which uses PZFlow\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PZFlowEstimator\n",
      " |      rail.estimation.estimator.CatEstimator\n",
      " |      rail.core.stage.RailStage\n",
      " |      ceci.stage.PipelineStage\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, args, comm=None)\n",
      " |      Initialize Estimator\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  config_options = {'calculated_point_estimates': [], 'chunk_size': 1000...\n",
      " |  \n",
      " |  inputs = [('model', <class 'rail.tools.flow_handle.FlowHandle'>), ('in...\n",
      " |  \n",
      " |  name = 'PZFlowEstimator'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from rail.estimation.estimator.CatEstimator:\n",
      " |  \n",
      " |  estimate(self, input_data)\n",
      " |      The main interface method for the photo-z estimation\n",
      " |      \n",
      " |      This will attach the input_data to this `Estimator`\n",
      " |      (for introspection and provenance tracking).\n",
      " |      \n",
      " |      Then it will call the run() and finalize() methods, which need to\n",
      " |      be implemented by the sub-classes.\n",
      " |      \n",
      " |      The run() method will need to register the data that it creates to this Estimator\n",
      " |      by using `self.add_data('output', output_data)`.\n",
      " |      \n",
      " |      Finally, this will return a QPHandle providing access to that output data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      input_data : `dict` or `ModelHandle`\n",
      " |          Either a dictionary of all input data or a `ModelHandle` providing access to the same\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      output: `QPHandle`\n",
      " |          Handle providing access to QP ensemble with output data\n",
      " |  \n",
      " |  open_model(self, **kwargs)\n",
      " |      Load the mode and/or attach it to this Estimator\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model : `object`, `str` or `ModelHandle`\n",
      " |          Either an object with a trained model,\n",
      " |          a path pointing to a file that can be read to obtain the trained model,\n",
      " |          or a `ModelHandle` providing access to the trained model.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.model : `object`\n",
      " |          The object encapsulating the trained model.\n",
      " |  \n",
      " |  run(self)\n",
      " |      Run the stage and return the execution status\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from rail.estimation.estimator.CatEstimator:\n",
      " |  \n",
      " |  outputs = [('output', <class 'rail.core.data.QPHandle'>)]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from rail.core.stage.RailStage:\n",
      " |  \n",
      " |  add_data(self, tag, data=None)\n",
      " |      Adds a handle to the DataStore associated to a particular tag and\n",
      " |      attaches data to it.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tag : str\n",
      " |          The tag (from cls.inputs or cls.outputs) for this data\n",
      " |      data : any\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      data : any\n",
      " |          The data accesed by the handle assocated to the tag\n",
      " |  \n",
      " |  add_handle(self, tag, data=None, path=None)\n",
      " |      Adds a DataHandle associated to a particular tag\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tag : str\n",
      " |          The tag (from cls.inputs or cls.outputs) for this data\n",
      " |      data : any or None\n",
      " |          If not None these data will be associated to the handle\n",
      " |      path : str or None\n",
      " |          If not None, this will be the path used to read the data\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      handle : DataHandle\n",
      " |          The handle that gives access to the associated data\n",
      " |  \n",
      " |  connect_input(self, other, inputTag=None, outputTag=None)\n",
      " |      Connect another stage to this stage as an input\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      other : RailStage\n",
      " |           The stage whose output is being connected\n",
      " |      inputTag : str\n",
      " |           Which input tag of this stage to connect to.  None -> self.inputs[0]\n",
      " |      outputTag : str\n",
      " |           Which output tag of the other stage to connect to.  None -> other.outputs[0]\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      handle : The input handle for this stage\n",
      " |  \n",
      " |  get_data(self, tag, allow_missing=True)\n",
      " |      Gets the data associated to a particular tag\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      1. This gets the data via the DataHandle, and can and will read the data\n",
      " |      from disk if needed.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tag : str\n",
      " |          The tag (from cls.inputs or cls.outputs) for this data\n",
      " |      allow_missing : bool\n",
      " |          If False this will raise a key error if the tag is not in the DataStore\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      data : any\n",
      " |          The data accesed by the handle assocated to the tag\n",
      " |  \n",
      " |  get_handle(self, tag, path=None, allow_missing=False)\n",
      " |      Gets a DataHandle associated to a particular tag\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tag : str\n",
      " |          The tag (from cls.inputs or cls.outputs) for this data\n",
      " |      path : str or None\n",
      " |          The path to the data, only needed if we might need to read the data\n",
      " |      allow_missing : bool\n",
      " |          If False this will raise a key error if the tag is not in the DataStore\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      handle : DataHandle\n",
      " |          The handle that give access to the associated data\n",
      " |  \n",
      " |  input_iterator(self, tag, **kwargs)\n",
      " |      Iterate the input assocated to a particular tag\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tag : str\n",
      " |          The tag (from cls.inputs or cls.outputs) for this data\n",
      " |      \n",
      " |      kwargs : dict[str, Any]\n",
      " |          These will be passed to the Handle's iterator method\n",
      " |  \n",
      " |  set_data(self, tag, data, path=None, do_read=True)\n",
      " |      Sets the data associated to a particular tag\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      1. If data is a DataHandle and tag is one of the input tags,\n",
      " |      then this will add an alias between the two, i.e., it will\n",
      " |      set `self.config.alias[tag] = data.tag`.  This allows the user to\n",
      " |      make connections between stages simply by passing DataHandles between\n",
      " |      them.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tag : str\n",
      " |          The tag (from cls.inputs or cls.outputs) for this data\n",
      " |      data : any\n",
      " |          The data being set,\n",
      " |      path : str or None\n",
      " |          Can be used to set the path for the data\n",
      " |      do_read : bool\n",
      " |          If True, will read the data if it is not set\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      data : any\n",
      " |          The data accesed by the handle assocated to the tag\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from rail.core.stage.RailStage:\n",
      " |  \n",
      " |  build(**kwargs) from builtins.type\n",
      " |      Return an object that can be used to build a stage\n",
      " |  \n",
      " |  make_and_connect(**kwargs) from builtins.type\n",
      " |      Make a stage and connects it to other stages\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      kwargs are used to set stage configuration,\n",
      " |      the should be key, value pairs, where the key\n",
      " |      is the parameter name and the value is value we want to assign\n",
      " |      \n",
      " |      The 'connections' keyword is special, it is a dict[str, DataHandle]\n",
      " |      and should define the Input connections for this stage\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A stage\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from rail.core.stage.RailStage:\n",
      " |  \n",
      " |  data_store = DataStore\n",
      " |  {  model:<class 'rail.tools.flow_handl....PqHan...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ceci.stage.PipelineStage:\n",
      " |  \n",
      " |  already_finished(self)\n",
      " |      Print a warning that a stage is being skipped\n",
      " |  \n",
      " |  check_io(self, args=None)\n",
      " |      Check the inputs and outputs.\n",
      " |      This function is seperate so that when Stages are configured interactively after\n",
      " |      construction then can invove this\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args: dict or namespace\n",
      " |          Specification of input and output paths and any missing config options\n",
      " |  \n",
      " |  data_ranges_by_rank(self, n_rows, chunk_rows, parallel=True)\n",
      " |      Split a number of rows by process.\n",
      " |      \n",
      " |      Given a total number of rows to read and a chunk size, yield\n",
      " |      the ranges within them that this process should handle.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n_rows: int\n",
      " |          Total number of rows to split up\n",
      " |      \n",
      " |      chunk_rows: int\n",
      " |          Size of each chunk to be read.\n",
      " |      \n",
      " |      Parallel: bool\n",
      " |          Whether to split data by rank or just give all procs all data.\n",
      " |          Default=True\n",
      " |  \n",
      " |  finalize(self)\n",
      " |      Finalize the stage, moving all its outputs to their final locations.\n",
      " |  \n",
      " |  find_inputs(self, pipeline_files)\n",
      " |      Find and retrun all the inputs associated to this stage in the FileManager\n",
      " |      \n",
      " |      These are returned as a dictionary of tag : path pairs\n",
      " |  \n",
      " |  find_outputs(self, outdir)\n",
      " |      Find and retrun all the outputs associated to this stage\n",
      " |      \n",
      " |      These are returned as a dictionary of tag : path pairs\n",
      " |  \n",
      " |  get_aliased_tag(self, tag)\n",
      " |      Returns the possibly remapped value for an input or output tag\n",
      " |      \n",
      " |      Parameter\n",
      " |      ---------\n",
      " |      tag : `str`\n",
      " |          The input or output tag we are checking\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      aliased_tag : `str`\n",
      " |          The aliases version of the tag\n",
      " |  \n",
      " |  get_aliases(self)\n",
      " |      Returns the dictionary of aliases used to remap inputs and outputs\n",
      " |      in the case that we want to have multiple instance of this class in the pipeline\n",
      " |  \n",
      " |  get_config_dict(self, ignore=None, reduce_config=False)\n",
      " |      Write the current configuration to a dict\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ignore : dict or None\n",
      " |          Global parameters not to write\n",
      " |      reduce_config : bool\n",
      " |          If true, reduce the configuration by parsing out the inputs, outputs and global params\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      out_dict : dict\n",
      " |          The configuration\n",
      " |  \n",
      " |  get_input(self, tag)\n",
      " |      Return the path of an input file with the given tag,\n",
      " |      which can be aliased.\n",
      " |  \n",
      " |  get_input_type(self, tag)\n",
      " |      Return the file type class of an input file with the given tag.\n",
      " |  \n",
      " |  get_output(self, tag, final_name=False)\n",
      " |      Return the path of an output file with the given tag,\n",
      " |      which can be aliased already.\n",
      " |      \n",
      " |      If final_name is False then use a temporary name - file will\n",
      " |      be moved to its final name at the end\n",
      " |  \n",
      " |  get_output_type(self, tag)\n",
      " |      Return the file type class of an output file with the given tag.\n",
      " |  \n",
      " |  is_dask(self)\n",
      " |      Returns True if the stage is being run in parallel with Dask.\n",
      " |  \n",
      " |  is_mpi(self)\n",
      " |      Returns True if the stage is being run under MPI.\n",
      " |  \n",
      " |  is_parallel(self)\n",
      " |      Returns True if the code is being run in parallel.\n",
      " |      Right now is_parallel() will return the same value as is_mpi(),\n",
      " |      but that may change in future if we implement other forms of\n",
      " |      parallelization.\n",
      " |  \n",
      " |  iterate_fits(self, tag, hdunum, cols, chunk_rows, parallel=True)\n",
      " |      Loop through chunks of the input data from a FITS file with the given tag\n",
      " |      \n",
      " |      TODO: add ceci tests of this functions\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tag: str\n",
      " |          The tag from the inputs list to use\n",
      " |      \n",
      " |      hdunum: int\n",
      " |          The extension number to read\n",
      " |      \n",
      " |      cols: list\n",
      " |          The columns to read\n",
      " |      \n",
      " |      chunk_rows: int\n",
      " |          Number of columns to read and return at once\n",
      " |      \n",
      " |      parallel: bool\n",
      " |          Whether to split up data among processes (parallel=True) or give\n",
      " |          all processes all data (parallel=False).  Default = True.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      it: iterator\n",
      " |          Iterator yielding (int, int, array) tuples of (start, end, data)\n",
      " |          data is a structured array.\n",
      " |  \n",
      " |  iterate_hdf(self, tag, group_name, cols, chunk_rows, parallel=True, longest=False)\n",
      " |      Loop through chunks of the input data from an HDF5 file with the given tag.\n",
      " |      \n",
      " |      All the selected columns must have the same length.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tag: str\n",
      " |          The tag from the inputs list to use\n",
      " |      \n",
      " |      group: str\n",
      " |          The group within the HDF5 file to use, looked up as\n",
      " |          file[group]\n",
      " |      \n",
      " |      cols: list\n",
      " |          The columns to read\n",
      " |      \n",
      " |      chunk_rows: int\n",
      " |          Number of columns to read and return at once\n",
      " |      \n",
      " |      parallel: bool\n",
      " |          Whether to split up data among processes (parallel=True) or give\n",
      " |          all processes all data (parallel=False).  Default = True.\n",
      " |      \n",
      " |      longest: bool\n",
      " |          Whether to allow mixed length arrays and keep going until the longest\n",
      " |          array is completed, returning empty arrays for shorter ones\n",
      " |      \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      it: iterator\n",
      " |          Iterator yielding (int, int, dict) tuples of (start, end, data)\n",
      " |  \n",
      " |  load_configs(self, args)\n",
      " |      Load the configuraiton\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args: dict or namespace\n",
      " |          Specification of input and output paths and any missing config options\n",
      " |  \n",
      " |  map_tasks_by_rank(self, function, inputs, allgather=False)\n",
      " |      Run a function over a series of inputs, in parallel\n",
      " |      \n",
      " |      This mirrors the map function, and returns the equivalent of\n",
      " |      [function(input) for input in inputs], but executes in parallel.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      function: Callable\n",
      " |          Function to be run on each item in inputs\n",
      " |      \n",
      " |      inputs: Iterable\n",
      " |          Any sequence of inputs, which should be the same\n",
      " |          on all processes. Or at least the same length:\n",
      " |          inputs not assigned to this process are ignored so\n",
      " |          you could get away with a dummy input for them.\n",
      " |      \n",
      " |      allgather: bool\n",
      " |          Whether to give all ranks the results (True) or just the\n",
      " |          root process (False). Default = False.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      results: list\n",
      " |          A list of the results of calling the function on each input,\n",
      " |          in the same order as the input tasks\n",
      " |  \n",
      " |  open_input(self, tag, wrapper=False, **kwargs)\n",
      " |      Find and open an input file with the given tag, in read-only mode.\n",
      " |      \n",
      " |      For general files this will simply return a standard\n",
      " |      python file object.\n",
      " |      \n",
      " |      For specialized file types like FITS or HDF5 it will return\n",
      " |      a more specific object - see the types.py file for more info.\n",
      " |  \n",
      " |  open_output(self, tag, wrapper=False, final_name=False, **kwargs)\n",
      " |      Find and open an output file with the given tag, in write mode.\n",
      " |      \n",
      " |      If final_name is True then they will be opened using their final\n",
      " |      target output name.  Otherwise we will prepend \"inprogress_\" to their\n",
      " |      file name. This means we know that if the final file exists then it\n",
      " |      is completed.\n",
      " |      \n",
      " |      If wrapper is True this will return an instance of the class\n",
      " |      of the file as specified in the cls.outputs.  Otherwise it will\n",
      " |      return an open file object (standard python one or something more\n",
      " |      specialized).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      tag: str\n",
      " |          Tag as listed in self.outputs\n",
      " |      \n",
      " |      wrapper: bool\n",
      " |          Default=False.  Whether to return a wrapped file\n",
      " |      \n",
      " |      final_name: bool\n",
      " |          Default=False. Whether to save to\n",
      " |      \n",
      " |      **kwargs:\n",
      " |          Extra args are passed on to the file's class constructor.\n",
      " |  \n",
      " |  print_io(self, stream=<ipykernel.iostream.OutStream object at 0x111cebfd0>)\n",
      " |      Print out the tags, paths and types for all the inputs and outputs of this stage\n",
      " |  \n",
      " |  read_config(self, args)\n",
      " |      This function looks for the arguments of the pipeline stage using a\n",
      " |      combination of default values, command line options and separate\n",
      " |      configuration file.\n",
      " |      \n",
      " |      The order for resolving config options is first looking for a default\n",
      " |      value, then looking for a\n",
      " |      \n",
      " |      In case a mandatory argument (argument with no default) is missing,\n",
      " |      an exception is raised.\n",
      " |      \n",
      " |      Note that we recognize arguments with no default as the ones where\n",
      " |      self.config_options holds a type instead of a value.\n",
      " |  \n",
      " |  setup_mpi(self, comm=None)\n",
      " |      Setup the MPI interface\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      comm: MPI communicator\n",
      " |          (default is None) An MPI comm object to use in preference to COMM_WORLD\n",
      " |  \n",
      " |  should_skip(self, run_config)\n",
      " |      Return true if we should skip a stage b/c it's outputs already exist and we are in resume mode\n",
      " |  \n",
      " |  split_tasks_by_rank(self, tasks)\n",
      " |      Iterate through a list of items, yielding ones this process is responsible for/\n",
      " |      \n",
      " |      Tasks are allocated in a round-robin way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tasks: iterable\n",
      " |          Tasks to split up\n",
      " |  \n",
      " |  start_dask(self)\n",
      " |      Prepare dask to run under MPI. After calling this method\n",
      " |      only a single process, MPI rank 1 will continue to exeute code\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from ceci.stage.PipelineStage:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      Python 3.6+ provides a facility to automatically\n",
      " |      call a method (this one) whenever a new subclass\n",
      " |      is defined.  In this case we use that feature to keep\n",
      " |      track of all available pipeline stages, each of which is\n",
      " |      defined by a class.\n",
      " |  \n",
      " |  execute(args, comm=None) from builtins.type\n",
      " |      Create an instance of this stage and run it\n",
      " |      with the specified inputs and outputs.\n",
      " |      \n",
      " |      This is calld by the main method.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      args: namespace\n",
      " |          The argparse namespace for this subclass.\n",
      " |  \n",
      " |  generate_command(inputs, config, outputs, aliases=None, instance_name=None) from builtins.type\n",
      " |      Generate a command line that will run the stage\n",
      " |  \n",
      " |  generate_cwl(log_dir=None) from builtins.type\n",
      " |      Produces a CWL App object which can then be exported to yaml\n",
      " |  \n",
      " |  get_module() from builtins.type\n",
      " |      Return the path to the python package containing the current sub-class\n",
      " |      \n",
      " |      If we have a PipelineStage subclass defined in a module called \"bar\", in\n",
      " |      a package called \"foo\" e.g.:\n",
      " |      /path/to/foo/bar.py  <--   contains subclass \"Baz\"\n",
      " |      \n",
      " |      Then calling Baz.get_module() will return \"foo.bar\".\n",
      " |      \n",
      " |      We use this later to construct command lines like \"python -m foo Baz\"\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      module: str\n",
      " |          The module containing this class.\n",
      " |  \n",
      " |  get_stage(name, module_name=None) from builtins.type\n",
      " |      Return the PipelineStage subclass with the given name.\n",
      " |      \n",
      " |      This is used so that we do not need a new entry point __main__ function\n",
      " |      for each new stage - instead we can just use a single one which can query\n",
      " |      which class it should be using based on the name.\n",
      " |      \n",
      " |      If module_name is provided, this will import that module\n",
      " |      in order to load the required class.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cls: class\n",
      " |          The corresponding subclass\n",
      " |  \n",
      " |  input_tags() from builtins.type\n",
      " |      Return the list of input tags required by this stage\n",
      " |  \n",
      " |  inputs_() from builtins.type\n",
      " |      Return the dict of inputs\n",
      " |  \n",
      " |  main() from builtins.type\n",
      " |      Create an instance of this stage and execute it with\n",
      " |      inputs and outputs taken from the command line\n",
      " |  \n",
      " |  make_stage(**kwargs) from builtins.type\n",
      " |      Make a stage of a particular type\n",
      " |  \n",
      " |  output_tags() from builtins.type\n",
      " |      Return the list of output tags required by this stage\n",
      " |  \n",
      " |  outputs_() from builtins.type\n",
      " |      Return the dict of inputs\n",
      " |  \n",
      " |  parse_command_line(cmd=None) from builtins.type\n",
      " |      Set up and argument parser and parse the command line\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cmd : str or None\n",
      " |          The command line to part (if None this will use the system arguments)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      args : Namespace\n",
      " |          The resulting Mapping of arguement to values\n",
      " |  \n",
      " |  usage() from builtins.type\n",
      " |      Print a usage message.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from ceci.stage.PipelineStage:\n",
      " |  \n",
      " |  stop_dask()\n",
      " |      End the dask event loop\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from ceci.stage.PipelineStage:\n",
      " |  \n",
      " |  comm\n",
      " |      The MPI communicator object (None if not running under MPI)\n",
      " |  \n",
      " |  config\n",
      " |      Returns the configuration dictionary for this stage, aggregating command\n",
      " |      line options and optional configuration file.\n",
      " |  \n",
      " |  instance_name\n",
      " |      Return the name associated to this particular instance of this stage\n",
      " |  \n",
      " |  rank\n",
      " |      The rank of this process under MPI (0 if not running under MPI)\n",
      " |  \n",
      " |  size\n",
      " |      The number or processes under MPI (1 if not running under MPI)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from ceci.stage.PipelineStage:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from ceci.stage.PipelineStage:\n",
      " |  \n",
      " |  allow_reload = False\n",
      " |  \n",
      " |  dask_parallel = False\n",
      " |  \n",
      " |  doc = ''\n",
      " |  \n",
      " |  incomplete_pipeline_stages = {'CatInformer': (<class 'rail.estimation....\n",
      " |  \n",
      " |  parallel = True\n",
      " |  \n",
      " |  pipeline_stages = {'CatEstimator': (<class 'rail.estimation.estimator....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(PZFlowEstimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigF(pivotz, ntrain, ntest, seed1, seed2, seed3, nbins):\n",
    "    ##things you need\n",
    "    #grid = makeGrid(0, 2.5, nbins) \n",
    "    bands = ['u','g','r','i','z','y']\n",
    "    band_dict = {band: f\"mag_{band}_lsst\" for band in bands}\n",
    "    band_dict_err = {f'mag_{band}_lsst_err':f'mag_err_{band}_lsst' for band in bands}\n",
    "\n",
    "    # modelData = makeModel()\n",
    "    \n",
    "    ##stages \n",
    "    trainData = trainSet(ntrain, seed1)\n",
    "    invRed = invRedshift(pivotz)\n",
    "\n",
    "    # origTrainPosts = getPosts(output_train_set.pq (???), modelData, grid)\n",
    "    # degTrainPosts = getPosts(###)\n",
    "\n",
    "    testData = testSet(ntest, seed2)\n",
    "    lsstErr = lsstError(band_dict, seed3)\n",
    "\n",
    "    # origTestPosts = getPosts(###)\n",
    "    # degTestPosts = getPosts(###)\n",
    "\n",
    "    # informFZB = informFZBoost()\n",
    "    # estFZB = estimateFZBoost(informFZB, nbins)\n",
    "\n",
    "    infPZFlow = informPZFlow()\n",
    "    estPZFlow = estimatePZFlow(infPZFlow)\n",
    "\n",
    "    \n",
    "    ##pipeline and yml\n",
    "    pipe = ceci.Pipeline.interactive()\n",
    "    stages = [\n",
    "        trainData, \n",
    "        invRed, \n",
    "        testData, \n",
    "        lsstErr,  \n",
    "        infPZFlow, \n",
    "        estPZFlow]\n",
    "        #informFZB, \n",
    "        #estFZB]\n",
    "    \n",
    "    for stage in stages:\n",
    "        pipe.add_stage(stage)\n",
    "        \n",
    "\n",
    "    invRed.connect_input(trainData)\n",
    "    lsstErr.connect_input(testData)\n",
    "\n",
    "    infPZFlow.connect_input(invRed) \n",
    "    estPZFlow.connect_input(infPZFlow, inputTag = 'model')\n",
    "    estPZFlow.connect_input(lsstErr, inputTag = 'input') ## trucated out of docs :(\n",
    "\n",
    "    # informFZB.connect_input(invRed)\n",
    "    # estFZB.connect_input(informFZB, lsstErr) \n",
    "    \n",
    "    pipe.initialize(\n",
    "    dict(model=flow_file), dict(output_dir=\".\", log_dir=\".\", resume=False), None) \n",
    "\n",
    "    return pipe.save(\"test_pipeline_est.yml\") \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigF(1.0, 100, 100, 17, 39, 172, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This code is from a demo I found; I did not write it \n",
    "\n",
    "\n",
    "# Python program to explain os.mkdir() method\n",
    "  \n",
    "# importing os module\n",
    "import os\n",
    "  \n",
    "# Directory\n",
    "directory = \"GeeksforGeeks\"\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"D:/Pycharm projects/\"\n",
    "\n",
    "# Path\n",
    "path = os.path.join(parent_dir, directory)\n",
    "  \n",
    "# Create the directory\n",
    "# 'GeeksForGeeks' in\n",
    "# '/home / User / Documents'\n",
    "os.mkdir(path)\n",
    "print(\"Directory '% s' created\" % directory)\n",
    "\n",
    "##############################################\n",
    "\n",
    "path = \"/Users/alicec03/Desktop/Summer_Research/Photo-z-Stress-Test/Photo-z-Stress-Test\" \n",
    "\n",
    "file = 'myfile.txt'\n",
    "  \n",
    "# Before creating\n",
    "dir_list = os.listdir(path) \n",
    "print(\"List of directories and files before creation:\")\n",
    "print(dir_list)\n",
    "print()\n",
    "  \n",
    "# Creating a file at specified location\n",
    "with open(os.path.join(path, file), 'w') as fp:\n",
    "    pass\n",
    "    # To write data to new file uncomment\n",
    "    # this fp.write(\"New file created\")\n",
    "  \n",
    "# After creating \n",
    "dir_list = os.listdir(path)\n",
    "print(\"List of directories and files after creation:\")\n",
    "print(dir_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this code will use my actual pipeline outputs when it works \n",
    "\n",
    "for i in range(0, 5, 0.1):\n",
    "    dir = \"invz_lsstErr_pzflow\"\n",
    "    parent_dir = \"D:/Pipeline_Outputs/\"\n",
    "    path = os.path.join(parent_dir, dir)\n",
    "    os.makedirs(path)\n",
    "        \n",
    "    file = bigF(i, 100, 100, 17, 39, 172, 10)\n",
    "    dir_path = \"/Users/alicec03/Desktop/Summer_Research/Photo-z-Stress-Test/Photo-z-Stress-Test/Pipeline_Outputs/% s\" % dir \n",
    "    with open(os.path.join(dir_path, file), 'w') as fp:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:/Users/alicec03/Desktop/Summer_Research/Photo-z-Stress-Test/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz=12345_lsstErr_pzflow.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m dir_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mD:/Users/alicec03/Desktop/Summer_Research/Photo-z-Stress-Test/Photo-z-Stress-Test/Pipeline_Outputs/\u001b[39m\u001b[39m% s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mdir\u001b[39m \n\u001b[1;32m      9\u001b[0m file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39minvz=\u001b[39m\u001b[39m% s\u001b[39;00m\u001b[39m_lsstErr_pzflow.txt\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m i\n\u001b[0;32m---> 10\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(dir_path, file), \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fp:\n\u001b[1;32m     11\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/rail---new/lib/python3.11/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:/Users/alicec03/Desktop/Summer_Research/Photo-z-Stress-Test/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz=12345_lsstErr_pzflow.txt'"
     ]
    }
   ],
   "source": [
    "## this is the code that I have a question about \n",
    "\n",
    "\n",
    "i = 12345\n",
    "\n",
    "dir = \"invz_lsstErr_pzflow\"\n",
    "parent_dir = \"D:/Pipeline_Outputs/\"\n",
    "path = os.path.join(parent_dir, dir)\n",
    "os.makedirs(path)\n",
    "        \n",
    "dir_path = \"D:/Users/alicec03/Desktop/Summer_Research/Photo-z-Stress-Test/Photo-z-Stress-Test/Pipeline_Outputs/% s\" % dir \n",
    "file = \"invz=% s_lsstErr_pzflow.txt\" % i\n",
    "with open(os.path.join(dir_path, file), 'w') as fp:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Executing test_set\n",
      "Command is:\n",
      "OMP_NUM_THREADS=1   python3 -m ceci rail.creation.engines.flowEngine.FlowCreator   --model=/Users/alicec03/Desktop/Summer_Research/Photo-z-Stress-Test/Photo-z-Stress-Test/data/trained_flow.pkl   --name=test_set   --config=test_pipeline_est_config.yml   --output=./output_test_set.pq \n",
      "Output writing to ./test_set.out\n",
      "\n",
      "Job test_set has completed successfully!\n",
      "\n",
      "Executing lsst_error\n",
      "Command is:\n",
      "OMP_NUM_THREADS=1   python3 -m ceci rail.creation.degradation.lsst_error_model.LSSTErrorModel   --input=./output_test_set.pq   --name=lsst_error   --config=test_pipeline_est_config.yml   --output=./output_lsst_error.pq \n",
      "Output writing to ./lsst_error.out\n",
      "\n",
      "Job lsst_error has completed successfully!\n",
      "\n",
      "Executing train_set\n",
      "Command is:\n",
      "OMP_NUM_THREADS=1   python3 -m ceci rail.creation.engines.flowEngine.FlowCreator   --model=/Users/alicec03/Desktop/Summer_Research/Photo-z-Stress-Test/Photo-z-Stress-Test/data/trained_flow.pkl   --name=train_set   --config=test_pipeline_est_config.yml   --output=./output_train_set.pq \n",
      "Output writing to ./train_set.out\n",
      "\n",
      "Job train_set has completed successfully!\n",
      "\n",
      "Executing inv_redshift\n",
      "Command is:\n",
      "OMP_NUM_THREADS=1   python3 -m ceci rail.creation.degradation.spectroscopic_degraders.InvRedshiftIncompleteness   --input=./output_train_set.pq   --name=inv_redshift   --config=test_pipeline_est_config.yml   --output=./output_inv_redshift.pq \n",
      "Output writing to ./inv_redshift.out\n",
      "\n",
      "Job inv_redshift has completed successfully!\n",
      "\n",
      "Executing inform_PZFlow\n",
      "Command is:\n",
      "OMP_NUM_THREADS=1   python3 -m ceci rail.estimation.algos.pzflow_nf.Inform_PZFlowPdf   --input=./output_inv_redshift.pq   --name=inform_PZFlow   --config=test_pipeline_est_config.yml   --model=./pzflow.pkl \n",
      "Output writing to ./inform_PZFlow.out\n",
      "\n",
      "Job inform_PZFlow has completed successfully!\n",
      "\n",
      "Executing estimate_PZFlow\n",
      "Command is:\n",
      "OMP_NUM_THREADS=1   python3 -m ceci rail.estimation.algos.pzflow_nf.PZFlowEstimator   --model=./pzflow.pkl   --input=./output_lsst_error.pq   --name=estimate_PZFlow   --config=test_pipeline_est_config.yml   --output=./output_estimate_PZFlow.hdf5 \n",
      "Output writing to ./estimate_PZFlow.out\n",
      "\n",
      "Job estimate_PZFlow has completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = ceci.Pipeline.read(\"test_pipeline_est.yml\")\n",
    "pr.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## more config parameters/better config parameters\n",
    "## have to give path above to estimator model instead of get_handle('model')\n",
    "## fix truncated parameter printing in help(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rail",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
