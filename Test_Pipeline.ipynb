{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rail.estimation.algos.knnpz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrail\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflexzboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FlexZBoostInformer, FlexZBoostEstimator\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrail\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgpz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPzEstimator, GPzEstimator \n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrail\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mknnpz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Inform_KNearNeighPDF, Inform_KNearNeighPDF \n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrail\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mminisom_som\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MiniSOMInformer, MiniSOMInformer \n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrail\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgos\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpzflow_nf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PZFlowInformer, PZFlowEstimator \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rail.estimation.algos.knnpz'"
     ]
    }
   ],
   "source": [
    "## Essential Imports: \n",
    "import os\n",
    "import numpy as np\n",
    "import qp\n",
    "import tables_io\n",
    "from pathlib import Path \n",
    "from pzflow.examples import get_galaxy_data\n",
    "import ceci\n",
    "\n",
    "## RAIL-Specific Imports: \n",
    "import rail\n",
    "\n",
    "# old : from rail.creation.degradation import LSSTErrorModel, InvRedshiftIncompleteness\n",
    "\n",
    "\n",
    "from rail.creation.degradation.lsst_error_model import LSSTErrorModel\n",
    "from rail.creation.degradation.spectroscopic_degraders import InvRedshiftIncompleteness\n",
    "\n",
    "import rail.creation \n",
    "import rail.creation.engines\n",
    "from rail.creation.engines.flowEngine import FlowModeler, FlowCreator, FlowPosterior\n",
    "from rail.core.data import TableHandle\n",
    "from rail.core.stage import RailStage\n",
    "from rail.core.utilStages import ColumnMapper, TableConverter\n",
    "\n",
    "# old : from rail.estimation.algos.flexzboost import Inform_FZBoost, FZBoost\n",
    "\n",
    "from rail.estimation.algos.pzflow_nf import PZFlowInformer, PZFlowEstimator \n",
    "from rail.estimation.algos.flexzboost import FlexZBoostInformer, FlexZBoostEstimator\n",
    "from rail.estimation.algos.gpz import GPzInformer, GPzEstimator\n",
    "from rail.estimation.algos.cmnn import CMNNInformer, CMNNEstimator \n",
    "from rail.estimation.algos.train_z import TrainZEstimator, TrainZInformer  \n",
    "# from rail.estimation.algos.k_nearneigh import #Inform_KNearNeighPDF, Inform_KNearNeighPDF \n",
    "#from rail.estimation.algos.minisom_som import MiniSOMInformer, MiniSOMEstimator \n",
    "#from rail.estimation.algos.sklearn_neurnet import #Inform_SimpleNN, Inform_SimpleNN \n",
    "#from rail.estimation.algos.somoclu_som import SOMocluInformer, SOMocluInformer\n",
    "\n",
    "#from rail.estimation.algos.bpz_lite import BPZliteInformer, BPZliteEstimator\n",
    "\n",
    "\n",
    "from rail.evaluation.evaluator import Evaluator\n",
    "\n",
    "\n",
    "## Data Storage: \n",
    "DS = RailStage.data_store\n",
    "DS.__class__.allow_overwrite = True\n",
    "\n",
    "\n",
    "### CMNN, PZFlow, FlexZBoost, GPZ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package rail.estimation.algos in rail.estimation:\n",
      "\n",
      "NAME\n",
      "    rail.estimation.algos\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _gpz_util\n",
      "    bpz_lite\n",
      "    cmnn\n",
      "    delightPZ\n",
      "    equal_count\n",
      "    flexzboost\n",
      "    gpz\n",
      "    k_nearneigh\n",
      "    minisom_som\n",
      "    naive_stack\n",
      "    nz_dir\n",
      "    point_est_hist\n",
      "    pzflow_nf\n",
      "    random_forest\n",
      "    random_gauss\n",
      "    sklearn_neurnet\n",
      "    somoclu_som\n",
      "    train_z\n",
      "    uniform_binning\n",
      "    var_inf\n",
      "\n",
      "FILE\n",
      "    (built-in)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(rail.estimation.algos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package rail.creation.engines in rail.creation:\n",
      "\n",
      "NAME\n",
      "    rail.creation.engines\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    dsps_photometry_creator\n",
      "    dsps_sed_modeler\n",
      "    flowEngine\n",
      "    gcr_engine\n",
      "\n",
      "FILE\n",
      "    (built-in)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(rail.creation.engines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from rail.stages import *\n",
    "#rail.stages.import_and_attach_all()\n",
    "#for val in RailStage.pipeline_stages.values():\n",
    "#    print(val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeModel():\n",
    "    #path to access the data \n",
    "    DATA_DIR = Path().resolve() / \"data\"\n",
    "    DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "    catalog_file = DATA_DIR / \"base_catalog.pq\"\n",
    "\n",
    "    bands = ['u','g','r','i','z','y']\n",
    "    band_dict = {band:f'mag_{band}_lsst' for band in bands}\n",
    "    \n",
    "    #array of galaxies w/ 7 attributes for each: redshift & ugrizy\n",
    "    catalog = get_galaxy_data().rename(band_dict, axis=1) \n",
    "\n",
    "    #turns array into a table \n",
    "    tables_io.write(catalog, str(catalog_file.with_suffix(\"\")), catalog_file.suffix[1:])\n",
    "\n",
    "    catalog_file = str(catalog_file)\n",
    "    flow_file = str(DATA_DIR / \"trained_flow.pkl\")\n",
    "\n",
    "    print(flow_file)\n",
    "\n",
    "    #we set up the stage \n",
    "    flow_modeler_params = {\n",
    "        \"name\": \"flow_modeler\",\n",
    "        \"input\": catalog_file,\n",
    "        \"model\": flow_file,\n",
    "        \"seed\": 0,\n",
    "        \"phys_cols\": {\"redshift\": [0, 3]},\n",
    "        \"phot_cols\": {\n",
    "            \"mag_u_lsst\": [17, 35],\n",
    "            \"mag_g_lsst\": [16, 32],\n",
    "            \"mag_r_lsst\": [15, 30],\n",
    "            \"mag_i_lsst\": [15, 30],\n",
    "            \"mag_z_lsst\": [14, 29],\n",
    "            \"mag_y_lsst\": [14, 28],\n",
    "        },\n",
    "        \"calc_colors\": {\"ref_column_name\": \"mag_i_lsst\"},\n",
    "    }\n",
    "    flow_modeler = FlowModeler.make_stage(**flow_modeler_params)\n",
    "    # flow_modeler.fit_model()\n",
    "    return flow_modeler, flow_file ##.get_handle(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/u2/a/acraffor/Photo-z-Stress-Test/data/trained_flow.pkl\n"
     ]
    }
   ],
   "source": [
    "modelData, flow_file = makeModel() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Training Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSet(ntrain, seed):\n",
    "    data = FlowCreator.make_stage(\n",
    "            name = 'train_set',\n",
    "            model = flow_file,\n",
    "            n_samples = ntrain,\n",
    "            seed = seed \n",
    "    )\n",
    "    return data #.sample(ntrain, seed)\n",
    "\n",
    "def invRedshift(pivot = 1.0):\n",
    "    degr = InvRedshiftIncompleteness.make_stage(\n",
    "        name = 'inv_redshift',\n",
    "        pivot_redshift = pivot\n",
    "    )\n",
    "    return degr #(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = FlowCreator.make_stage(\n",
    "#             name = 'train_set',\n",
    "#             model = flow_file,\n",
    "#             n_samples = 2,\n",
    "#             seed = 78 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origTrainData = trainSet(modelData, 100, 372)\n",
    "# bubble = origTrainData.sample(100, 372)\n",
    "\n",
    "# degTrainData = invRedshift(1.0)\n",
    "# dot = degTrainData(bubble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# degTrainData.get_handle('output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPosts(data, model, grid):\n",
    "    posts = FlowPosterior.make_stage(\n",
    "        name='get_posts'+str(data), \n",
    "        column='redshift',\n",
    "        grid = grid,\n",
    "        model = model,\n",
    "        data = data\n",
    "    )\n",
    "    return posts #posts.get_posterior(data, column = 'redshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeGrid(zmin, zmax, nbins):\n",
    "    import numpy as np\n",
    "    grid = np.linspace(zmin, zmax, nbins + 1)\n",
    "    return grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = makeGrid(0, 2.5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origTrainPosts = getPosts(origTrainData, modelData, grid)\n",
    "# degTrainPosts = getPosts(degTrainData, modelData, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run if you need output_orig_train_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_post_orig_train = FlowPosterior.make_stage(name='orig_train_posts', \n",
    "#                                              column='redshift',\n",
    "#                                              grid = np.linspace(0, 2.5, 101),\n",
    "#                                              model=flow_file,\n",
    "#                                              data = orig_train)\n",
    "\n",
    "# orig_train_pdfs = flow_post_orig_train.get_posterior(orig_train, column='redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run if you need output_deg_train_posts ** rerun this cell!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_post_deg_train = FlowPosterior.make_stage(name='deg_train_posts', \n",
    "#                                              column='redshift',\n",
    "#                                              grid = np.linspace(0, 2.5, 101),\n",
    "#                                              model=flow_file,\n",
    "#                                              err_samples = 0,\n",
    "#                                              data = deg_train)\n",
    "\n",
    "\n",
    "\n",
    "# deg_train_pdfs = flow_post_deg_train.get_posterior(deg_train, column='redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Test Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testSet(ntest, seed):\n",
    "    data = FlowCreator.make_stage(\n",
    "            name = 'test_set',\n",
    "            model = flow_file,\n",
    "            n_samples = ntest,\n",
    "            seed = seed \n",
    "    )\n",
    "    return data #.sample(ntest, seed)\n",
    "\n",
    "\n",
    "## you need to ask alex about where you can find the defaults for these params \n",
    "\n",
    "bands = ['u','g','r','i','z','y']\n",
    "band_dict = {band:f'mag_{band}_lsst' for band in bands}\n",
    "\n",
    "def lsstError(dict, seed): #tvis = 1, nYrObs = 1, airmass = 1, extendedSource = 1, sigmaSys = 1, magLim = 1, ndFlag = 1, A_min = 1, A_max = 1):\n",
    "    deg = LSSTErrorModel.make_stage(\n",
    "        name='lsst_error',\n",
    "        renameDict= dict, \n",
    "        ndFlag=np.nan,\n",
    "        seed=seed,\n",
    "    )\n",
    "    return deg #(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testSetMaker = testSet(modelData, 100, 17)\n",
    "# testData = testSetMaker.sample(100, 17)\n",
    "# degTestData = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run if you need output_orig_test_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_post_orig_test = FlowPosterior.make_stage(name='orig_test_posts', \n",
    "#                                              column='redshift',\n",
    "#                                              grid = np.linspace(0, 2.5, 101),\n",
    "#                                              model=flow_file,\n",
    "#                                              data = orig_test)\n",
    "\n",
    "# orig_test_pdfs = flow_post_orig_test.get_posterior(orig_test, column='redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only run if you need output_deg_test_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_post_deg_test = FlowPosterior.make_stage(name='deg_test_posts', \n",
    "#                                              column='redshift',\n",
    "#                                              grid = np.linspace(0, 2.5, 101),\n",
    "#                                              model=flow_file,\n",
    "#                                              data = deg_test)\n",
    "\n",
    "# deg_test_pdfs = flow_post_deg_test.get_posterior(deg_test, column='redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Make tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def makeTable(datafile):\n",
    "    \n",
    "#     bands = ['u','g','r','i','z','y']\n",
    "#     rename_dict = {f'mag_{band}_lsst_err':f'mag_err_{band}_lsst' for band in bands}\n",
    "\n",
    "#     col_remapper = ColumnMapper.make_stage(\n",
    "#     name='col_remapper', \n",
    "#     columns=rename_dict,\n",
    "#     )\n",
    "#     table_conv = TableConverter.make_stage(\n",
    "#     name='table_conv', \n",
    "#     output_format='numpyDict',\n",
    "#     )\n",
    "#     pq = col_remapper(datafile)\n",
    "#     tabledata = table_conv(pq)\n",
    "#     table = tables_io.convertObj(tabledata.data, tables_io.types.PD_DATAFRAME)\n",
    "#     return table\n",
    "\n",
    "\n",
    "# ## make two separate functions for each stage, make bands, rename_dict inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands = ['u','g','r','i','z','y']\n",
    "band_dict_err = {f'mag_{band}_lsst_err':f'mag_err_{band}_lsst' for band in bands}\n",
    "\n",
    "def colRemapper(dict):\n",
    "    col_remap = ColumnMapper.make_stage(\n",
    "    name='col_remapper', \n",
    "    columns=dict,\n",
    "    )\n",
    "    return col_remap\n",
    "\n",
    "def tableConverter():\n",
    "    table_conv = TableConverter.make_stage(\n",
    "    name='table_conv', \n",
    "    output_format='numpyDict',\n",
    "    )\n",
    "    return table_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_remap = colRemapper(band_dict_err)\n",
    "table_conv = tableConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# squiggle = colRemapper(band_dict_err)\n",
    "# noodle = tableConverter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainTable = makeTable(trainData)\n",
    "# testTable = makeTable(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Inform & Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informFZBoost():\n",
    "    info = Inform_FZBoost.make_stage(\n",
    "    name ='inform_FZBoost', \n",
    "    model ='fzboost.pkl', \n",
    "    hdf5_groupname='',\n",
    "    )\n",
    "    # info.inform(data)\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# informedEst = informFZBoost()\n",
    "# informedEst.inform(degTrainData.get_handle('output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimateFZBoost(info, nbins):\n",
    "    est = FZBoost.make_stage(\n",
    "    name='est_FZBoost', \n",
    "    nondetect_val=np.nan,\n",
    "    model= info, #.get_handle('model'), \n",
    "    hdf5_groupname='',\n",
    "    aliases=dict(input='test_data', output='fzboost_estim'),\n",
    "    nzbins = nbins \n",
    "    )\n",
    "    return est #.estimate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estData = estimateFZBoost(informedEst, 100)\n",
    "\n",
    "# estData.estimate(testSetMaker.get_handle('output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function connect_input in module rail.core.stage:\n",
      "\n",
      "connect_input(self, other, inputTag=None, outputTag=None)\n",
      "    Connect another stage to this stage as an input\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    other : RailStage\n",
      "         The stage whose output is being connected\n",
      "    inputTag : str\n",
      "         Which input tag of this stage to connect to.  None -> self.inputs[0]\n",
      "    outputTag : str\n",
      "         Which output tag of the other stage to connect to.  None -> other.outputs[0]\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    handle : The input handle for this stage\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rail.core import RailStage\n",
    "\n",
    "help(RailStage.connect_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def informPZFlow():\n",
    "    inf = PZFlowInformer.make_stage(\n",
    "    name = 'inform_PZFlow',\n",
    "    model = 'pzflow.pkl',\n",
    "    hdf5_groupname=\"\"\n",
    "    )\n",
    "    return inf\n",
    "\n",
    "def estimatePZFlow(info):\n",
    "    est = PZFlowEstimator.make_stage(\n",
    "    name = 'estimate_PZFlow',\n",
    "    model = 'pzflow.pkl', #info.get_handle('model'),\n",
    "    hdf5_groupname=\"\"\n",
    "    )\n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(PZFlowEstimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Survey-Based Degraders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rail.creation.degradation.spectroscopic_selections import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rail.creation.degradation.spectroscopic_selections.SpecSelection_BOSS\n",
    "rail.creation.degradation.spectroscopic_selections.SpecSelection_DEEP2\n",
    "rail.creation.degradation.spectroscopic_selections.SpecSelection_GAMA\n",
    "rail.creation.degradation.spectroscopic_selections.SpecSelection_HSC\n",
    "rail.creation.degradation.spectroscopic_selections.SpecSelection_VVDSf02 \n",
    "rail.creation.degradation.spectroscopic_selections.SpecSelection_zCOSMOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def specSelectBOSS(ntrain):\n",
    "    degr = SpecSelection_BOSS.make_stage(\n",
    "        name = 'specselection_boss',\n",
    "        N_tot = ntrain\n",
    "    )\n",
    "    return degr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def specSelectDEEP2(ntrain):\n",
    "    degr = SpecSelection_DEEP2.make_stage(\n",
    "        name = 'specselection_deep2',\n",
    "        N_tot = ntrain\n",
    "    )\n",
    "    return degr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def specSelectGAMA(ntrain):\n",
    "    degr = SpecSelection_GAMA.make_stage(\n",
    "        name = 'specselection_gama',\n",
    "        N_tot = ntrain\n",
    "    )\n",
    "    return degr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def specSelectHSC(ntrain):\n",
    "    degr = SpecSelection_HSC.make_stage(\n",
    "        name = 'specselection_HSC',\n",
    "        N_tot = ntrain\n",
    "    )\n",
    "    return degr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def specSelectVVDSf02(ntrain):\n",
    "    degr = SpecSelection_VVDSf02.make_stage(\n",
    "        name = 'specselection_VVDSf02',\n",
    "        N_tot = ntrain\n",
    "    )\n",
    "    return degr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def specSelectzCOSMOS(ntrain):\n",
    "    degr = SpecSelection_zCOSMOS.make_stage(\n",
    "        name = 'specselection_zCOSMOS',\n",
    "        N_tot = ntrain\n",
    "    )\n",
    "    return degr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Big F 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigF1(pivotz, ntrain, ntest, seed1, seed2, seed3, nbins):\n",
    "    \n",
    "    ##things you need\n",
    "    #grid = makeGrid(0, 2.5, nbins) \n",
    "    bands = ['u','g','r','i','z','y']\n",
    "    band_dict = {band: f\"mag_{band}_lsst\" for band in bands}\n",
    "    band_dict_err = {f'mag_{band}_lsst_err':f'mag_err_{band}_lsst' for band in bands}\n",
    "\n",
    "    # modelData = makeModel()\n",
    "    \n",
    "    ##stages \n",
    "    trainData = trainSet(ntrain, seed1)\n",
    "    invRed = invRedshift(pivotz)\n",
    "\n",
    "    # origTrainPosts = getPosts(output_train_set.pq (???), modelData, grid)\n",
    "    # degTrainPosts = getPosts(###)\n",
    "\n",
    "    testData = testSet(ntest, seed2)\n",
    "    lsstErr = lsstError(band_dict, seed3)\n",
    "\n",
    "    # origTestPosts = getPosts(###)\n",
    "    # degTestPosts = getPosts(###)\n",
    "\n",
    "    # informFZB = informFZBoost()\n",
    "    # estFZB = estimateFZBoost(informFZB, nbins)\n",
    "\n",
    "    infPZFlow = informPZFlow()\n",
    "    estPZFlow = estimatePZFlow(infPZFlow)\n",
    "\n",
    "    \n",
    "    ##pipeline and yml\n",
    "    pipe = ceci.Pipeline.interactive()\n",
    "    stages = [\n",
    "        trainData, \n",
    "        invRed, \n",
    "        testData, \n",
    "        lsstErr,  \n",
    "        infPZFlow, \n",
    "        estPZFlow]\n",
    "        #informFZB, \n",
    "        #estFZB]\n",
    "    \n",
    "    for stage in stages:\n",
    "        pipe.add_stage(stage)\n",
    "        \n",
    "\n",
    "    invRed.connect_input(trainData)\n",
    "    lsstErr.connect_input(testData)\n",
    "\n",
    "    infPZFlow.connect_input(invRed) \n",
    "    estPZFlow.connect_input(infPZFlow, inputTag = 'model')\n",
    "    estPZFlow.connect_input(lsstErr, inputTag = 'input') ## trucated out of docs :(\n",
    "\n",
    "    # informFZB.connect_input(invRed)\n",
    "    # estFZB.connect_input(informFZB, lsstErr) \n",
    "    \n",
    "    pipe.initialize(\n",
    "    dict(model=flow_file), dict(output_dir=\".\", log_dir=\".\", resume=False), None) \n",
    "\n",
    "    outpath = os.path.join(path_1, \"invz='% s'_lsstErr_pzflow.yml\" % '%.3f'%(pivotz))\n",
    "    pipe.save(outpath)\n",
    "    return outpath \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big F 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bigF2(ntrain, ntest, seed1, seed2, seed3, nbins):\n",
    "    \n",
    "    ##things you need\n",
    "    #grid = makeGrid(0, 2.5, nbins) \n",
    "    bands = ['u','g','r','i','z','y']\n",
    "    band_dict = {band: f\"mag_{band}_lsst\" for band in bands}\n",
    "    band_dict_err = {f'mag_{band}_lsst_err':f'mag_err_{band}_lsst' for band in bands}\n",
    "\n",
    "    # modelData = makeModel()\n",
    "    \n",
    "    ##stages \n",
    "    \n",
    "    trainData = trainSet(ntrain, seed1)\n",
    "    \n",
    "    deg_ls = [specSelectBOSS(ntrain), \n",
    "          specSelectDEEP2(ntrain), \n",
    "          specSelectGAMA(ntrain), \n",
    "          specSelectHSC(ntrain), \n",
    "          specSelectVVDSf02(ntrain), \n",
    "          specSelectzCOSMOS(ntrain)]\n",
    "    \n",
    "    name_ls = ['BOSS', 'DEEP2', 'GAMA', 'HSC', 'VVDSf02', 'zCOSMOS']\n",
    "\n",
    "    for i in range(len(deg_ls)):\n",
    "        \n",
    "        deg = deg_ls[i]\n",
    "\n",
    "        testData = testSet(ntest, seed2)\n",
    "        lsstErr = lsstError(band_dict, seed3)\n",
    "\n",
    "        infPZFlow = informPZFlow()\n",
    "        estPZFlow = estimatePZFlow(infPZFlow)\n",
    "\n",
    "\n",
    "        ##pipeline and yml\n",
    "        pipe = ceci.Pipeline.interactive()\n",
    "        stages = [\n",
    "            trainData, \n",
    "            deg, \n",
    "            testData, \n",
    "            lsstErr,  \n",
    "            infPZFlow, \n",
    "            estPZFlow]\n",
    "            #informFZB, \n",
    "            #estFZB]\n",
    "\n",
    "        for stage in stages:\n",
    "            pipe.add_stage(stage)\n",
    "\n",
    "\n",
    "        deg.connect_input(trainData)\n",
    "        lsstErr.connect_input(testData)\n",
    "\n",
    "        infPZFlow.connect_input(deg) \n",
    "        estPZFlow.connect_input(infPZFlow, inputTag = 'model')\n",
    "        estPZFlow.connect_input(lsstErr, inputTag = 'input') ## trucated out of docs :(\n",
    "\n",
    "        # informFZB.connect_input(invRed)\n",
    "        # estFZB.connect_input(informFZB, lsstErr) \n",
    "\n",
    "        pipe.initialize(\n",
    "        dict(model=flow_file), dict(output_dir=\".\", log_dir=\".\", resume=False), None) \n",
    "\n",
    "        outpath = os.path.join(path_2, \"'% s'_lsstErr_pzflow.yml\" % name_ls[i])\n",
    "        pipe.save(outpath)\n",
    "        return outpath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# help(rail.creation.degradation.spectroscopic_selections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'informPZFlow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m path_2 \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(parent_dir, directory)\n\u001b[1;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(path_2)\n\u001b[0;32m----> 9\u001b[0m path_lst_2\u001b[38;5;241m.\u001b[39mappend(\u001b[43mbigF2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m39\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m172\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[44], line 31\u001b[0m, in \u001b[0;36mbigF2\u001b[0;34m(ntrain, ntest, seed1, seed2, seed3, nbins)\u001b[0m\n\u001b[1;32m     28\u001b[0m testData \u001b[38;5;241m=\u001b[39m testSet(ntest, seed2)\n\u001b[1;32m     29\u001b[0m lsstErr \u001b[38;5;241m=\u001b[39m lsstError(band_dict, seed3)\n\u001b[0;32m---> 31\u001b[0m infPZFlow \u001b[38;5;241m=\u001b[39m \u001b[43minformPZFlow\u001b[49m()\n\u001b[1;32m     32\u001b[0m estPZFlow \u001b[38;5;241m=\u001b[39m estimatePZFlow(infPZFlow)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m##pipeline and yml\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'informPZFlow' is not defined"
     ]
    }
   ],
   "source": [
    "##run \n",
    "\n",
    "path_lst_2 = []\n",
    "directory = \"specSelection_lsstErr_pzflow\"\n",
    "parent_dir = \"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs\"\n",
    "path_2 = os.path.join(parent_dir, directory)\n",
    "os.makedirs(path_2)\n",
    "\n",
    "path_lst_2.append(bigF2(100000, 100000, 17, 39, 172, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Choose Pivot z's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "## seed1 and ndata should be the same as  seed1 and ntrain used to call bigF!! \n",
    "## Otherwise this might not be representative of the real data \n",
    "\n",
    "def choosePivots(seed1, ndata):\n",
    "    nums = trainSet(ndata, seed1)\n",
    "    data = nums.sample(ndata, seed1)\n",
    "    data_pq = col_remap(data)\n",
    "    data_table = table_conv(data_pq)\n",
    "    table = tables_io.convertObj(data_table.data, tables_io.types.PD_DATAFRAME)\n",
    "    return np.asarray(table['redshift'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting handle into data store.  model: /global/u2/a/acraffor/Photo-z-Stress-Test/data/trained_flow.pkl, train_set\n",
      "Inserting handle into data store.  output_train_set: inprogress_output_train_set.pq, train_set\n",
      "Inserting handle into data store.  output_col_remapper: inprogress_output_col_remapper.pq, col_remapper\n",
      "Inserting handle into data store.  output_table_conv: inprogress_output_table_conv.hdf5, table_conv\n",
      "Inserting handle into data store.  output_train_set: inprogress_output_train_set.pq, train_set\n",
      "Inserting handle into data store.  output_col_remapper: inprogress_output_col_remapper.pq, col_remapper\n",
      "Inserting handle into data store.  output_table_conv: inprogress_output_table_conv.hdf5, table_conv\n",
      "Inserting handle into data store.  output_train_set: inprogress_output_train_set.pq, train_set\n",
      "Inserting handle into data store.  output_col_remapper: inprogress_output_col_remapper.pq, col_remapper\n",
      "Inserting handle into data store.  output_table_conv: inprogress_output_table_conv.hdf5, table_conv\n",
      "Inserting handle into data store.  output_train_set: inprogress_output_train_set.pq, train_set\n",
      "Inserting handle into data store.  output_col_remapper: inprogress_output_col_remapper.pq, col_remapper\n",
      "Inserting handle into data store.  output_table_conv: inprogress_output_table_conv.hdf5, table_conv\n",
      "Inserting handle into data store.  output_train_set: inprogress_output_train_set.pq, train_set\n",
      "Inserting handle into data store.  output_col_remapper: inprogress_output_col_remapper.pq, col_remapper\n",
      "Inserting handle into data store.  output_table_conv: inprogress_output_table_conv.hdf5, table_conv\n",
      "Inserting handle into data store.  output_train_set: inprogress_output_train_set.pq, train_set\n",
      "Inserting handle into data store.  output_col_remapper: inprogress_output_col_remapper.pq, col_remapper\n",
      "Inserting handle into data store.  output_table_conv: inprogress_output_table_conv.hdf5, table_conv\n",
      "Inserting handle into data store.  output_train_set: inprogress_output_train_set.pq, train_set\n",
      "Inserting handle into data store.  output_col_remapper: inprogress_output_col_remapper.pq, col_remapper\n",
      "Inserting handle into data store.  output_table_conv: inprogress_output_table_conv.hdf5, table_conv\n",
      "Inserting handle into data store.  output_train_set: inprogress_output_train_set.pq, train_set\n",
      "Inserting handle into data store.  output_col_remapper: inprogress_output_col_remapper.pq, col_remapper\n",
      "Inserting handle into data store.  output_table_conv: inprogress_output_table_conv.hdf5, table_conv\n",
      "Inserting handle into data store.  output_train_set: inprogress_output_train_set.pq, train_set\n",
      "Inserting handle into data store.  output_col_remapper: inprogress_output_col_remapper.pq, col_remapper\n",
      "Inserting handle into data store.  output_table_conv: inprogress_output_table_conv.hdf5, table_conv\n"
     ]
    }
   ],
   "source": [
    "percentiles = np.arange(10, 100, 10)\n",
    "pivots = [] \n",
    "\n",
    "for i in percentiles:\n",
    "    pivot = np.percentile(choosePivots(17, 100000), i) \n",
    "    pivots.append(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33672517538070684, 0.47006111145019536, 0.6267686605453491, 0.8275491118431091, 1.0106754302978516, 1.2042927742004392, 1.4413679003715512, 1.6783331394195558, 1.9954649806022646]\n"
     ]
    }
   ],
   "source": [
    "print(pivots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Run Big F 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting handle into data store.  output_test_set: inprogress_output_test_set.pq, test_set\n",
      "Inserting handle into data store.  output_inv_redshift: inprogress_output_inv_redshift.pq, inv_redshift\n",
      "Inserting handle into data store.  model_inform_PZFlow: inprogress_pzflow.pkl, inform_PZFlow\n",
      "Inserting handle into data store.  output_lsst_error: inprogress_output_lsst_error.pq, lsst_error\n"
     ]
    }
   ],
   "source": [
    "path_lst_1 = []\n",
    "directory = \"invz_lsstErr_pzflow\"\n",
    "parent_dir = \"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs\"\n",
    "path_1 = os.path.join(parent_dir, directory)\n",
    "os.makedirs(path_1)\n",
    "\n",
    "for i in (pivots):\n",
    "    path_lst.append(bigF(i, 100000, 100000, 17, 39, 172, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz='0.33672517538070684'_lsstErr_pzflow.yml\", \"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz='0.47006111145019536'_lsstErr_pzflow.yml\", \"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz='0.6267686605453491'_lsstErr_pzflow.yml\", \"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz='0.8275491118431091'_lsstErr_pzflow.yml\", \"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz='1.0106754302978516'_lsstErr_pzflow.yml\", \"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz='1.2042927742004392'_lsstErr_pzflow.yml\", \"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz='1.4413679003715512'_lsstErr_pzflow.yml\", \"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz='1.6783331394195558'_lsstErr_pzflow.yml\", \"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz='1.9954649806022646'_lsstErr_pzflow.yml\"]\n"
     ]
    }
   ],
   "source": [
    "print(path_lst_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConstructorError",
     "evalue": "could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:numpy.core.multiarray.scalar'\n  in \"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz='0.33672517538070684'_lsstErr_pzflow_config.yml\", line 76, column 19",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConstructorError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pr \u001b[38;5;241m=\u001b[39m \u001b[43mceci\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_lst\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#parent_dir+directory+\"/invz=0.33672517538070684_lsstErr_pzflow.yml\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pr\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m## 1) terminal: go to path up to invz_lsstErr_pzflow, then run these 2 lines \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m## 2)  make list/txt file with list of paths to files made by big F\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m## %cd ? \u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/rail-dev/lib/python3.10/site-packages/ceci/pipeline.py:488\u001b[0m, in \u001b[0;36mPipeline.read\u001b[0;34m(cls, pipeline_config_filename, extra_config, dry_run)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m    487\u001b[0m         \u001b[38;5;28m__import__\u001b[39m(module)\n\u001b[0;32m--> 488\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipe_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rail-dev/lib/python3.10/site-packages/ceci/pipeline.py:386\u001b[0m, in \u001b[0;36mPipeline.create\u001b[0;34m(pipe_config)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m    380\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown pipeline launcher \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlauncher_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, options are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(launcher_dict\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    381\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmsg\u001b[39;00m\n\u001b[1;32m    383\u001b[0m p \u001b[38;5;241m=\u001b[39m pipeline_class(\n\u001b[1;32m    384\u001b[0m     stages, launcher_config, overall_inputs\u001b[38;5;241m=\u001b[39minputs, modules\u001b[38;5;241m=\u001b[39mmodules\n\u001b[1;32m    385\u001b[0m )\n\u001b[0;32m--> 386\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstages_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/.conda/envs/rail-dev/lib/python3.10/site-packages/ceci/pipeline.py:790\u001b[0m, in \u001b[0;36mPipeline.initialize\u001b[0;34m(self, overall_inputs, run_config, stages_config)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages_config) \u001b[38;5;28;01mas\u001b[39;00m stage_config_file:\n\u001b[0;32m--> 790\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_config_data \u001b[38;5;241m=\u001b[39m \u001b[43myaml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage_config_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage_config_data \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.conda/envs/rail-dev/lib/python3.10/site-packages/yaml/__init__.py:125\u001b[0m, in \u001b[0;36msafe_load\u001b[0;34m(stream)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_load\u001b[39m(stream):\n\u001b[1;32m    118\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m    Parse the first YAML document in a stream\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m    and produce the corresponding Python object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    to be safe for untrusted input.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSafeLoader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rail-dev/lib/python3.10/site-packages/yaml/__init__.py:81\u001b[0m, in \u001b[0;36mload\u001b[0;34m(stream, Loader)\u001b[0m\n\u001b[1;32m     79\u001b[0m loader \u001b[38;5;241m=\u001b[39m Loader(stream)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_single_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     loader\u001b[38;5;241m.\u001b[39mdispose()\n",
      "File \u001b[0;32m~/.conda/envs/rail-dev/lib/python3.10/site-packages/yaml/constructor.py:51\u001b[0m, in \u001b[0;36mBaseConstructor.get_single_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_single_node()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/rail-dev/lib/python3.10/site-packages/yaml/constructor.py:60\u001b[0m, in \u001b[0;36mBaseConstructor.construct_document\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_generators \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m generator \u001b[38;5;129;01min\u001b[39;00m state_generators:\n\u001b[0;32m---> 60\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m dummy \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m     61\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstructed_objects \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.conda/envs/rail-dev/lib/python3.10/site-packages/yaml/constructor.py:413\u001b[0m, in \u001b[0;36mSafeConstructor.construct_yaml_map\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    411\u001b[0m data \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m data\n\u001b[0;32m--> 413\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m data\u001b[38;5;241m.\u001b[39mupdate(value)\n",
      "File \u001b[0;32m~/.conda/envs/rail-dev/lib/python3.10/site-packages/yaml/constructor.py:218\u001b[0m, in \u001b[0;36mSafeConstructor.construct_mapping\u001b[0;34m(self, node, deep)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, MappingNode):\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten_mapping(node)\n\u001b[0;32m--> 218\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/rail-dev/lib/python3.10/site-packages/yaml/constructor.py:143\u001b[0m, in \u001b[0;36mBaseConstructor.construct_mapping\u001b[0;34m(self, node, deep)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mHashable):\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConstructorError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile constructing a mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m, node\u001b[38;5;241m.\u001b[39mstart_mark,\n\u001b[1;32m    142\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound unhashable key\u001b[39m\u001b[38;5;124m\"\u001b[39m, key_node\u001b[38;5;241m.\u001b[39mstart_mark)\n\u001b[0;32m--> 143\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m     mapping[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mapping\n",
      "File \u001b[0;32m~/.conda/envs/rail-dev/lib/python3.10/site-packages/yaml/constructor.py:100\u001b[0m, in \u001b[0;36mBaseConstructor.construct_object\u001b[0;34m(self, node, deep)\u001b[0m\n\u001b[1;32m     98\u001b[0m             constructor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_mapping\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tag_suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 100\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mconstructor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     data \u001b[38;5;241m=\u001b[39m constructor(\u001b[38;5;28mself\u001b[39m, tag_suffix, node)\n",
      "File \u001b[0;32m~/.conda/envs/rail-dev/lib/python3.10/site-packages/yaml/constructor.py:427\u001b[0m, in \u001b[0;36mSafeConstructor.construct_undefined\u001b[0;34m(self, node)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_undefined\u001b[39m(\u001b[38;5;28mself\u001b[39m, node):\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConstructorError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    428\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not determine a constructor for the tag \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m node\u001b[38;5;241m.\u001b[39mtag,\n\u001b[1;32m    429\u001b[0m             node\u001b[38;5;241m.\u001b[39mstart_mark)\n",
      "\u001b[0;31mConstructorError\u001b[0m: could not determine a constructor for the tag 'tag:yaml.org,2002:python/object/apply:numpy.core.multiarray.scalar'\n  in \"/global/u2/a/acraffor/Photo-z-Stress-Test/Pipeline_Outputs/invz_lsstErr_pzflow/invz='0.33672517538070684'_lsstErr_pzflow_config.yml\", line 76, column 19"
     ]
    }
   ],
   "source": [
    "pr = ceci.Pipeline.read(path_lst_1[0])#parent_dir+directory+\"/invz=0.33672517538070684_lsstErr_pzflow.yml\")\n",
    "pr.run()\n",
    "\n",
    "## 1) terminal: go to path up to invz_lsstErr_pzflow, then run these 2 lines \n",
    "## 2)  make list/txt file with list of paths to files made by big F\n",
    "\n",
    "## do 1) \n",
    "## open virtual env\n",
    "## python \n",
    "## import ceci \n",
    "## run the 2 lines of code above \n",
    "\n",
    "\n",
    "### at the end we can put this into a .py file that we can run at the command line \n",
    "\n",
    "## %cd ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## more config parameters/better config parameters\n",
    "## have to give path above to estimator model instead of get_handle('model')\n",
    "## fix truncated parameter printing in help(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rail-dev",
   "language": "python",
   "name": "rail-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
